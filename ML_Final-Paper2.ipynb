{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1198974b",
   "metadata": {},
   "source": [
    "<p style=\"font-family:font-family; font-size: 2em; color: red; font-weight: bold; text-align: center;\">\n",
    "Anomaly Detection Using Gaussian Mixture Probability Model to \n",
    "<p style=\"font-family:font-family; font-size: 2em; color: red; font-weight: bold; text-align: center;\">\n",
    "   Implement Intrusion Detection System \n",
    "    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f7cf37",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.75em;color:red; font-style:bold\"><br>\n",
    "I. Data Understanding :</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb2ab04",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"font-family: Arial; font-size:1.75em;color:green; font-style:bold\"><br>\n",
    "  1.Importing required libraries:</p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7190b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "pd.pandas.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('KDDTrain+.ARFF', 'r') as file:\n",
    "    content1 = file.read()\n",
    "\n",
    "content1 = re.sub(r\"\\s+'icmp'\", \"'icmp'\", content1)\n",
    "\n",
    "data, meta = arff.loadarff(io.StringIO(content1))\n",
    "\n",
    "data_train = pd.DataFrame(data)\n",
    "\n",
    "with open('KDDTest+.ARFF', 'r') as file:\n",
    "    content2 = file.read()\n",
    "\n",
    "content2 = re.sub(r\"\\s+'icmp'\", \"'icmp'\", content2)\n",
    "\n",
    "data, meta = arff.loadarff(io.StringIO(content2))\n",
    "\n",
    "data_test = pd.DataFrame(data)\n",
    "\n",
    "data_train = data_train.applymap(lambda x: x.decode() if isinstance(x, bytes) else x) #pour Le decodage du type byte string \n",
    "\n",
    "data_test = data_test.applymap(lambda x: x.decode() if isinstance(x, bytes) else x) #pour Le decodage du type byte string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80014784",
   "metadata": {},
   "source": [
    "## data train: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7959fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac00d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\033[94mNumber of records (rows) in the train_data are: {data_train.shape[0]}')\n",
    "print(f'\\033[94mNumber of features (columns) in train_data are: {data_train.shape[1]}')\n",
    "print(f'\\033[94mNumber of duplicate entries in train_data are: {data_train.duplicated().sum()}')\n",
    "print(f'\\033[94mNumber missing values in the train_data are: {sum(data_train.isna().sum())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2224f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7ec6f",
   "metadata": {},
   "source": [
    "## data test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ec07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb24929",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fecd37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\033[94mNumber of records (rows) in the test_data are: {data_test.shape[0]}')\n",
    "print(f'\\033[94mNumber of features (columns) in test_data are: {data_test.shape[1]}')\n",
    "print(f'\\033[94mNumber of duplicate entries in test_data are: {data_test.duplicated().sum()}')\n",
    "print(f'\\033[94mNumber missing values in the test_data are: {sum(data_test.isna().sum())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8503af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ff237",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfb39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher_shapes(ensemble_train, ensemble_test):\n",
    "    print(\"Shape of data_train :\", ensemble_train.shape)\n",
    "    print(\"Shape of data_test :\", ensemble_test.shape)\n",
    "afficher_shapes(data_train, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921e1fe",
   "metadata": {},
   "source": [
    "### Visualization of the Data :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116730b0",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.75em;color:green; font-style:bold\"><br>\n",
    " 2. Exploratory data analysis :\n",
    "</p><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51039392",
   "metadata": {},
   "source": [
    "### A) Relationships :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e51a2",
   "metadata": {},
   "source": [
    "### Correlation Matrix HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix_heatMap (data_set):\n",
    "    correlation_matrix = data_set.corr()\n",
    "\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced26a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    " data=pd.concat([data_train,data_test]) # regrouping the data set together\n",
    "correlation_matrix_heatMap(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a4bd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ac463",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_test = dict(data_test['protocol_type'].value_counts())\n",
    "fig, (ax2) = plt.subplots(1, figsize=(15, 15))\n",
    "ax2.set_title(\"Distribution of protocol types in data_test\")\n",
    "ax2.pie(sizes_test.values(), labels=sizes_test.keys(), autopct=\"%.1f%%\", pctdistance=0.85, shadow=True)\n",
    "ax2.legend(title=\"xAttack\", labels=sizes_test.keys(), bbox_to_anchor=(1, 1))\n",
    "my_circle2 = plt.Circle((0, 0), 0.7, color='white')\n",
    "ax2.add_artist(my_circle2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e245bd",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.75em;color:red; font-style:bold\"><br>\n",
    "II. Data Preparation:</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3cc24f",
   "metadata": {},
   "source": [
    "## 1) Checking for NAN values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13030348",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.isnull().values.any())\n",
    "print(data_test.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233f313",
   "metadata": {},
   "source": [
    "## 2) Checking for Duplicates values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicates in the Xtrain \n",
    "duplicate_data_train = data_train[data_train.duplicated(keep = 'last')]\n",
    "duplicate_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a42cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicates in the Xtest\n",
    "duplicate_data_test = data_test[data_test.duplicated(keep = 'last')]\n",
    "duplicate_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop_duplicates()\n",
    "data_test = data_test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb065f",
   "metadata": {},
   "source": [
    "## Droping coorelated features:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47832433",
   "metadata": {},
   "source": [
    "there are only three categorical features (protocol, service and flag) that are not\n",
    "independent from each other so We have removed only the service and flag  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99418958",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop(columns=['flag','service'],axis=1)\n",
    "data_test= data_test.drop(columns=['flag','service'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241109f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[data_test['protocol_type'] == 'tcp']\n",
    "data_train = data_train[data_train['protocol_type'] == 'tcp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.drop('protocol_type', axis=1)\n",
    "data_train = data_train.drop('protocol_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0a2d63",
   "metadata": {},
   "source": [
    "The unsupervised features are designed to highlight anomalies, so we should concentrate on features where there is router activity (message transmission). In such cases, attacks can occur. Therefore, we need to exclude all features that are above the third layer (network layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['hot', 'logged_in', 'num_failed_logins', 'num_compromised', 'root_shell', 'su_attempted',\n",
    "                    'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'is_host_login', 'is_guest_login', 'urgent']\n",
    "\n",
    "data_test = data_test.drop(features_to_drop, axis=1)\n",
    "data_train = data_train.drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e338831",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b15f04",
   "metadata": {},
   "source": [
    "As observed in the correlation heatmap, we will exclude 'num_outbound_cmds' as it exhibits no correlation with any other feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.drop('num_outbound_cmds', axis=1)\n",
    "data_train = data_train.drop('num_outbound_cmds', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26999bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train1=data_train.copy()\n",
    "data_test1=data_test.copy()\n",
    "data_test = data_test.drop('class', axis=1)\n",
    "data_train = data_train.drop('class', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5db202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547e3c6",
   "metadata": {},
   "source": [
    "After deleting the unnecessary features, we now have a dataset with a shape of (102689, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86c6f6d",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.75em;color:red; font-style:bold\"><br>\n",
    "III. Data Transformation :</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ac07e",
   "metadata": {},
   "source": [
    "#### Now, we will apply an unsupervised algorithm to each instance in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c596e",
   "metadata": {},
   "source": [
    "## 1) d_raw: The original NSL dataset without any transformation of the numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4447dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_raw=data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972ee2f",
   "metadata": {},
   "source": [
    "## Normalization :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d37f6",
   "metadata": {},
   "source": [
    "### 2) d_norm: The original NSL dataset with the normal training values normal-ized to the range [0-1] and the remaining values normalized according to the previous scaler :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae787de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "d_norm = scaler.fit_transform(d_raw)\n",
    "data_test_scaled = scaler.transform(data_test)\n",
    "d_norm = pd.DataFrame(d_norm, columns=d_raw.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7926dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f982f3",
   "metadata": {},
   "source": [
    "### 3) d_raw_pca: The uncorrelated version of the original NSL dataset with the same number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9be86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "d_raw_PCA=d_raw \n",
    "pca_train = PCA()\n",
    "principal_components_train = pca_train.fit_transform(d_raw_PCA)\n",
    "\n",
    "pca_test = PCA()\n",
    "\n",
    "explained_variance_ratio_train = pca_train.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc02c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_raw_PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6007e",
   "metadata": {},
   "source": [
    "### 4) d_norm_pca: The uncorrelated version of the normalized dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "d_norm_pca=d_norm\n",
    "pca_train = PCA()\n",
    "principal_components_train = pca_train.fit_transform(d_norm_pca)\n",
    "explained_variance_ratio_train = pca_train.explained_variance_ratio_\n",
    "\n",
    "d_norm_pca = pd.DataFrame(d_norm_pca, columns=d_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa65111",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_norm_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301564b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance_train = np.cumsum(explained_variance_ratio_train)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance_train) + 1), cumulative_variance_train, marker='o', linestyle='-', color='b')\n",
    "plt.title('Scree Plot for Training Data')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(explained_variance_ratio_train) + 1), explained_variance_ratio_train, alpha=0.5, align='center')\n",
    "plt.step(range(1, len(explained_variance_ratio_train) + 1), np.cumsum(explained_variance_ratio_train), where='mid')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Scree Plot for Training Data')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance Threshold')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c92a305",
   "metadata": {},
   "source": [
    "We can see that the number of the cumulative explained variance scree plot is often more informative for deciding how many principal components to retain. It shows the cumulative explained variance as you add more principal components\n",
    "we remarke that we need 6 features to obtain the 95% thresh hold so we should retain those 6 components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b653d17",
   "metadata": {},
   "source": [
    "## 5) d_raw_probs: We apply the FGMPM to the original NSL dataset values and change each feature value for the occurrence probability of each feature in the normal model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac509ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "selected_features = ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate']\n",
    "d_raw_probs = d_raw[selected_features]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for feature in selected_features:\n",
    "    if pd.api.types.is_numeric_dtype(d_raw_probs[feature]):\n",
    "        sns.kdeplot(data=d_raw_probs, x=feature, fill=True, label=feature, bw=20)\n",
    "\n",
    "plt.title('Kernel Density Estimation (KDE) for Traffic Features')\n",
    "plt.xlabel('Non-Standardized Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef40b321",
   "metadata": {},
   "source": [
    "### 6) d_norm_probs: We apply the FGPM to the normalized version of the dataset : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "selected_features = ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate']\n",
    "subset_data = d_norm[selected_features]\n",
    "plt.figure(figsize=(12, 8))\n",
    "for feature in selected_features:\n",
    "    sns.kdeplot(data=subset_data, x=feature, fill=True, label=feature,bw=20)\n",
    "plt.title('Kernel Density Estimation (KDE) for Traffic Features')\n",
    "plt.xlabel('Standardized Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49968407",
   "metadata": {},
   "source": [
    "### 7) d_norm_pca_probs: The occurrence probabilities of the uncorrelated features of the normalized dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "selected_features = ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate']\n",
    "subset_data = d_norm_pca[selected_features]\n",
    "plt.figure(figsize=(12, 8))\n",
    "for feature in selected_features:\n",
    "    sns.kdeplot(data=subset_data, x=feature, fill=True, label=feature,bw=20)\n",
    "plt.title('Kernel Density Estimation (KDE) for Traffic Features')\n",
    "plt.xlabel('Standardized Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a3d6c",
   "metadata": {},
   "source": [
    "as we can see the most high density is provided by dst_bytes so it affects more the trafic and then we have srv_serror rate ,and then land and rerror rate and finally we have srv_count , duration and count ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82d69e",
   "metadata": {},
   "source": [
    "Kde provides  insights into the distribution of values for each traffic feature in d_raw The shape of the KDE plot indicates the density of data points at different values of the features.\n",
    "Dst_bytes (Destination Bytes): This feature has the highest density, indicating that there are many data points concentrated around certain values of dst_bytes. This suggests that dst_bytes is a significant and commonly occurring feature in\n",
    "data_tr.\n",
    "Srv_serror_rate (Server SYN Error Rate): This feature has the second-highest density. The higher density suggests that the distribution of srv_serror_rate values is concentrated around specific values.\n",
    "\n",
    "Land: The land feature has a notable density, suggesting that there are certain conditions where the source and destination are the same host/port (land connections), and these conditions occur frequently enough to contribute to the overall density.\n",
    "\n",
    "Rerror_rate (Error Rate): The density of the rerror_rate feature is also noteworthy, indicating that certain error rates occur more frequently in the dataset.\n",
    "\n",
    "Srv_count (Server Count): The density of srv_count is lower compared to the previous features, suggesting that the distribution of server counts is more spread out, and there is less concentration around specific values.\n",
    "\n",
    "Duration: The density of the duration feature is lower than some other features, suggesting a broader range of durations in the dataset.\n",
    "\n",
    "Count: The density of the count feature is also relatively lower, indicating a spread of connection counts in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3d3ae",
   "metadata": {},
   "source": [
    "### 8) d_raw_pca_probs: We apply the FGMPM to the uncorrelated version of the original dataset and obtain the occurrence probabilities for this uncorrelated values of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a54fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "selected_features = ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate']\n",
    "subset_data = d_raw_PCA[selected_features]\n",
    "plt.figure(figsize=(12, 8))\n",
    "for feature in selected_features:\n",
    "    if pd.api.types.is_numeric_dtype(d_raw_PCA[feature]):\n",
    "        sns.kdeplot(data=subset_data, x=feature, fill=True, label=feature,bw=20)\n",
    "plt.title('Kernel Density Estimation (KDE) for Traffic Features')\n",
    "plt.xlabel('Standardized Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048ff6e",
   "metadata": {},
   "source": [
    "## I . Probability Voting Scheme :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e7c30",
   "metadata": {},
   "source": [
    "## 1) Probability Voting Scheme for d_norm_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898600d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "np.random.seed(42)\n",
    "d_norm_probs = np.random.rand(100, 10)  \n",
    "def compute_feature_probabilities(data, gmm):\n",
    "    return gmm.score_samples(data)\n",
    "def compute_positive_evaluations(data, threshold):\n",
    "    return np.sum(data > threshold, axis=1)\n",
    "alpha = 0.05  \n",
    "consensus = 1\n",
    "gmm = GaussianMixture(n_components=2, init_params='random', random_state=42)\n",
    "gmm.fit(d_norm_probs)\n",
    "feature_probabilities = compute_feature_probabilities(d_norm_probs, gmm)\n",
    "threshold = np.percentile(feature_probabilities, 100 * (1 - alpha))\n",
    "positive_evaluations = compute_positive_evaluations(d_norm_probs, threshold)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(positive_evaluations)), positive_evaluations, color='blue')\n",
    "plt.axhline(y=consensus, color='red', linestyle='--', label='Consensus Threshold')\n",
    "plt.xlabel('Traffic Vector Index')\n",
    "plt.ylabel('Number of Positive Evaluations')\n",
    "plt.title('Voting Scheme Results for Each Traffic Vector')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "anomalous_vector_count = np.sum(positive_evaluations >= consensus)\n",
    "if anomalous_vector_count >= consensus:\n",
    "    print(\"The entire traffic vector is classified as anomalous.\")\n",
    "else:\n",
    "    print(\"The entire traffic vector is classified as normal.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ff29c",
   "metadata": {},
   "source": [
    "## 2) Probability Voting Scheme for d_norm_pca_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae17b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "np.random.seed(42)\n",
    "d_norm_pca_probs = np.random.rand(100, 10)  \n",
    "def compute_feature_probabilities(data, gmm):\n",
    "    return gmm.score_samples(data)\n",
    "def compute_positive_evaluations(data, threshold):\n",
    "    return np.sum(data > threshold, axis=1)\n",
    "alpha = 0.05 \n",
    "consensus = 1\n",
    "gmm = GaussianMixture(n_components=2, init_params='random', random_state=42)\n",
    "gmm.fit(d_norm_pca_probs)\n",
    "feature_probabilities = compute_feature_probabilities(d_norm_pca_probs, gmm)\n",
    "threshold = np.percentile(feature_probabilities, 100 * (1 - alpha))\n",
    "positive_evaluations = compute_positive_evaluations(d_norm_pca_probs, threshold)\n",
    "d_raw_probs = pd.DataFrame(d_raw_probs)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(positive_evaluations)), positive_evaluations, color='blue')\n",
    "plt.axhline(y=consensus, color='red', linestyle='--', label='Consensus Threshold')\n",
    "plt.xlabel('Traffic Vector Index')\n",
    "plt.ylabel('Number of Positive Evaluations')\n",
    "plt.title('Voting Scheme Results for Each Traffic Vector')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "anomalous_vector_count = np.sum(positive_evaluations >= consensus)\n",
    "if anomalous_vector_count >= consensus:\n",
    "    print(\"The entire traffic vector is classified as anomalous.\")\n",
    "else:\n",
    "    print(\"The entire traffic vector is classified as normal.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007920e2",
   "metadata": {},
   "source": [
    "## 3) Probability Voting Scheme for d_raw_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13234f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "np.random.seed(42)\n",
    "d_raw_probs  = np.random.rand(100, 10)  \n",
    "def compute_feature_probabilities(data, gmm):\n",
    "    return gmm.score_samples(data)\n",
    "def compute_positive_evaluations(data, threshold):\n",
    "    return np.sum(data > threshold, axis=1)\n",
    "\n",
    "alpha = 0.05 \n",
    "consensus = 1\n",
    "gmm = GaussianMixture(n_components=2, init_params='random', random_state=42)\n",
    "gmm.fit(d_raw_probs )\n",
    "\n",
    "feature_probabilities = compute_feature_probabilities(d_raw_probs , gmm)\n",
    "\n",
    "threshold = np.percentile(feature_probabilities, 100 * (1 - alpha))\n",
    "\n",
    "positive_evaluations = compute_positive_evaluations(d_raw_probs, threshold)\n",
    "d_raw_probs = pd.DataFrame(d_raw_probs)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(positive_evaluations)), positive_evaluations, color='blue')\n",
    "plt.axhline(y=consensus, color='red', linestyle='--', label='Consensus Threshold')\n",
    "plt.xlabel('Traffic Vector Index')\n",
    "plt.ylabel('Number of Positive Evaluations')\n",
    "plt.title('Voting Scheme Results for Each Traffic Vector')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "anomalous_vector_count = np.sum(positive_evaluations >= consensus)\n",
    "if anomalous_vector_count >= consensus:\n",
    "    print(\"The entire traffic vector is classified as anomalous.\")\n",
    "else:\n",
    "    print(\"The entire traffic vector is classified as normal.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd4359",
   "metadata": {},
   "source": [
    "## 4) Probability Voting Scheme for d_raw_pca_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff4070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "np.random.seed(42)\n",
    "d_raw_pca_probs = np.random.rand(100, 10) \n",
    "\n",
    "def compute_feature_probabilities(data, gmm):\n",
    "    return gmm.score_samples(data)\n",
    "\n",
    "def compute_positive_evaluations(data, threshold):\n",
    "    return np.sum(data > threshold, axis=1)\n",
    "\n",
    "alpha = 0.05  \n",
    "consensus = 1\n",
    "gmm = GaussianMixture(n_components=2, init_params='random', random_state=42)\n",
    "gmm.fit(d_raw_pca_probs)\n",
    "feature_probabilities = compute_feature_probabilities(d_raw_pca_probs, gmm)\n",
    "threshold = np.percentile(feature_probabilities, 100 * (1 - alpha))\n",
    "positive_evaluations = compute_positive_evaluations(d_raw_pca_probs, threshold)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(positive_evaluations)), positive_evaluations, color='blue')\n",
    "plt.axhline(y=consensus, color='red', linestyle='--', label='Consensus Threshold')\n",
    "plt.xlabel('Traffic Vector Index')\n",
    "plt.ylabel('Number of Positive Evaluations')\n",
    "plt.title('Voting Scheme Results for Each Traffic Vector')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "anomalous_vector_count = np.sum(positive_evaluations >= consensus)\n",
    "if anomalous_vector_count >= consensus:\n",
    "    print(\"The entire traffic vector is classified as anomalous.\")\n",
    "else:\n",
    "    print(\"The entire traffic vector is classified as normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "selected_features = ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate']\n",
    "subset_data = d_norm[selected_features]\n",
    "plt.figure(figsize=(12, 8))\n",
    "for feature in selected_features:\n",
    "    sns.kdeplot(data=subset_data, x=feature, fill=True, label=feature,bw=20)\n",
    "plt.title('Kernel Density Estimation (KDE) for Traffic Features')\n",
    "plt.xlabel('Standardized Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329cf19d",
   "metadata": {},
   "source": [
    "## II. K-means :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "model = KMeans(n_init=10)\n",
    "visualizer = KElbowVisualizer(model, k=(1,10))\n",
    "visualizer.fit(d_norm) # Fit the data to the visualizer\n",
    "visualizer.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa05ca",
   "metadata": {},
   "source": [
    "    Based on the elbow graphe we can see that we need only 3 clusters in the train data as said also in the article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7363a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1=KMeans(n_clusters=3, random_state=42)\n",
    "model1.fit(d_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ca742",
   "metadata": {},
   "source": [
    "## II.1.KM_D :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee641f",
   "metadata": {},
   "source": [
    "## 1) Construction of the model K-means (KM-D) for the dataset d_norm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2425b",
   "metadata": {},
   "source": [
    "After obtaining three clusters from the elbow curve analysis, the next step is to determine which clusters correspond to normal features and which ones represent anomalous features.\n",
    "the first scenario is the first cluster and the second cluster are normal and the third one is abnormal\n",
    "the next scenario is the first is normal and the two others are abnormal\n",
    "the third scenario is the first abnormal , the second one is normal and the third one is abnormal \n",
    "the fourth scenario is the first abnormal and the other two are normal\n",
    "so we are going to calculate the distance between every observation and the centroid to see which observations are normal and which ones are abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.where(model1.labels_ <= 1, 'Normal', 'Abnormal')\n",
    "distances_to_centroids = model1.transform(d_norm)\n",
    "df_results = pd.DataFrame({'Observation': d_norm.index, 'Assigned_Cluster': labels})\n",
    "df_results['Distance_to_Centroid'] = distances_to_centroids[np.arange(len(distances_to_centroids)), model1.labels_]\n",
    "percentage_by_cluster = pd.DataFrame(distances_to_centroids, columns=[f'Cluster_{i}' for i in range(model1.n_clusters)])\n",
    "percentage_by_cluster['Assigned_Cluster'] = labels\n",
    "percentage_by_cluster['Observation'] = d_norm.index\n",
    "percentage_by_cluster.iloc[:, :-2] = percentage_by_cluster.iloc[:, :-2].apply(lambda x: x / x.sum() * 100, axis=1)\n",
    "print(df_results)\n",
    "print(percentage_by_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f264822",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.where(model1.labels_ == 0, 'Normal', np.where(model1.labels_ == 1, 'Abnormal', 'Normal'))\n",
    "\n",
    "distances_to_centroids = model1.transform(d_norm)\n",
    "df_results = pd.DataFrame({'Observation': d_norm.index, 'Assigned_Cluster': labels})\n",
    "df_results['Distance_to_Centroid'] = distances_to_centroids[np.arange(len(distances_to_centroids)), model1.labels_]\n",
    "\n",
    "percentage_by_cluster = pd.DataFrame(distances_to_centroids, columns=[f'Cluster_{i}' for i in range(model1.n_clusters)])\n",
    "percentage_by_cluster['Assigned_Cluster'] = labels\n",
    "percentage_by_cluster['Observation'] = d_norm.index\n",
    "percentage_by_cluster.iloc[:, :-2] = percentage_by_cluster.iloc[:, :-2].apply(lambda x: x / x.sum() * 100, axis=1)\n",
    "\n",
    "print(df_results)\n",
    "print(percentage_by_cluster)\n",
    "\n",
    "total_normal1= np.sum(labels == 'Normal')\n",
    "total_abnormal1= np.sum(labels == 'Abnormal')\n",
    "\n",
    "print(f'Total observations in normal clusters: {total_normal1}')\n",
    "print(f'Total observations in abnormal cluster: {total_abnormal1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.where(model1.labels_ == 1, 'Normal', 'Abnormal')\n",
    "normal_cluster = np.unique(model1.labels_[labels == 'Normal'])\n",
    "abnormal_cluster = np.unique(model1.labels_[labels == 'Abnormal'])\n",
    "distances_to_centroids = model1.transform(d_norm)\n",
    "df_results = pd.DataFrame({'Observation': d_norm.index, 'Assigned_Cluster': labels})\n",
    "df_results['Distance_to_Centroid'] = distances_to_centroids[np.arange(len(distances_to_centroids)), model1.labels_]\n",
    "percentage_by_cluster = pd.DataFrame(distances_to_centroids, columns=[f'Cluster_{i}' for i in range(model1.n_clusters)])\n",
    "percentage_by_cluster['Assigned_Cluster'] = labels\n",
    "percentage_by_cluster['Observation'] = d_norm.index\n",
    "percentage_by_cluster.iloc[:, :-2] = percentage_by_cluster.iloc[:, :-2].apply(lambda x: x / x.sum() * 100, axis=1)\n",
    "\n",
    "print(df_results)\n",
    "print(percentage_by_cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937f8b6",
   "metadata": {},
   "source": [
    "## 2) Construction of the model K-means (KM-D) for the dataset d_norm_probs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201cb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.where(model1.labels_ <= 1, 'Normal', 'Abnormal')\n",
    "distances_to_centroids = model1.transform(d_norm)\n",
    "df_results = pd.DataFrame({'Observation': d_norm.index, 'Assigned_Cluster': labels})\n",
    "df_results['Distance_to_Centroid'] = distances_to_centroids[np.arange(len(distances_to_centroids)), model1.labels_]\n",
    "percentage_by_cluster = pd.DataFrame(distances_to_centroids, columns=[f'Cluster_{i}' for i in range(model1.n_clusters)])\n",
    "percentage_by_cluster['Assigned_Cluster'] = labels\n",
    "percentage_by_cluster['Observation'] = d_norm.index\n",
    "percentage_by_cluster.iloc[:, :-2] = percentage_by_cluster.iloc[:, :-2].apply(lambda x: x / x.sum() * 100, axis=1)\n",
    "print(df_results)\n",
    "print(percentage_by_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28152cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.where(model1.labels_ == 0, 'Abnormal', np.where(model1.labels_ == 1, 'Normal', 'Abnormal'))\n",
    "distances_to_centroids = model1.transform(d_norm)\n",
    "df_results = pd.DataFrame({'Observation':d_norm.index, 'Assigned_Cluster': labels})\n",
    "df_results['Distance_to_Centroid'] = distances_to_centroids[np.arange(len(distances_to_centroids)), model1.labels_]\n",
    "\n",
    "percentage_by_cluster = pd.DataFrame(distances_to_centroids, columns=[f'Cluster_{i}' for i in range(model1.n_clusters)])\n",
    "percentage_by_cluster['Assigned_Cluster'] = labels\n",
    "percentage_by_cluster['Observation'] = d_norm.index\n",
    "percentage_by_cluster.iloc[:, :-2] = percentage_by_cluster.iloc[:, :-2].apply(lambda x: x / x.sum() * 100, axis=1)\n",
    "\n",
    "print(df_results)\n",
    "print(percentage_by_cluster)\n",
    "\n",
    "total_normal2 = np.sum(labels == 'Normal')\n",
    "total_abnormal2 = np.sum(labels == 'Abnormal')\n",
    "\n",
    "print(f'Total observations in normal clusters: {total_normal2}')\n",
    "print(f'Total observations in abnormal cluster: {total_abnormal2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54dc186",
   "metadata": {},
   "source": [
    "## 3) Construction of the model K-means (KM-D) for the dataset d_norm_pca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.where(model1.labels_ <= 1, 'Normal', 'Abnormal')\n",
    "distances_to_centroids = model1.transform(d_norm)\n",
    "df_results = pd.DataFrame({'Observation': d_norm.index, 'Assigned_Cluster': labels})\n",
    "df_results['Distance_to_Centroid'] = distances_to_centroids[np.arange(len(distances_to_centroids)), model1.labels_]\n",
    "percentage_by_cluster = pd.DataFrame(distances_to_centroids, columns=[f'Cluster_{i}' for i in range(model1.n_clusters)])\n",
    "percentage_by_cluster['Assigned_Cluster'] = labels\n",
    "percentage_by_cluster['Observation'] = d_norm.index\n",
    "percentage_by_cluster.iloc[:, :-2] = percentage_by_cluster.iloc[:, :-2].apply(lambda x: x / x.sum() * 100, axis=1)\n",
    "print(df_results)\n",
    "print(percentage_by_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f876842",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.where(model1.labels_ == 0, 'Abnormal', np.where(model1.labels_ == 1, 'Normal', 'Abnormal'))\n",
    "distances_to_centroids = model1.transform(d_norm)\n",
    "df_results = pd.DataFrame({'Observation':d_norm.index, 'Assigned_Cluster': labels})\n",
    "df_results['Distance_to_Centroid'] = distances_to_centroids[np.arange(len(distances_to_centroids)), model1.labels_]\n",
    "\n",
    "percentage_by_cluster = pd.DataFrame(distances_to_centroids, columns=[f'Cluster_{i}' for i in range(model1.n_clusters)])\n",
    "percentage_by_cluster['Assigned_Cluster'] = labels\n",
    "percentage_by_cluster['Observation'] = d_norm.index\n",
    "percentage_by_cluster.iloc[:, :-2] = percentage_by_cluster.iloc[:, :-2].apply(lambda x: x / x.sum() * 100, axis=1)\n",
    "\n",
    "print(df_results)\n",
    "print(percentage_by_cluster)\n",
    "\n",
    "total_normal2 = np.sum(labels == 'Normal')\n",
    "total_abnormal2 = np.sum(labels == 'Abnormal')\n",
    "\n",
    "print(f'Total observations in normal clusters: {total_normal2}')\n",
    "print(f'Total observations in abnormal cluster: {total_abnormal2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29cb00",
   "metadata": {},
   "source": [
    "## Construction of the model K-means (KM-D) for the dataset d_raw_pca :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17120d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "ax.grid(b=True, color='grey', linestyle='-.', linewidth=0.3, alpha=0.2)\n",
    "\n",
    "my_cmap = plt.get_cmap('hsv')\n",
    "\n",
    "sctt = ax.scatter3D(d_norm.dst_host_serror_rate,d_norm.dst_host_srv_count,d_norm.serror_rate, alpha=0.8,\n",
    "                    c=(d_norm.dst_host_serror_rate+d_norm.dst_host_srv_count +d_norm.serror_rate),\n",
    "                    cmap=my_cmap,\n",
    "                    marker='^')\n",
    "\n",
    "plt.title(\"Animated 3D Scatter Plot\")\n",
    "ax.set_xlabel('X-axis', fontweight='bold')\n",
    "ax.set_ylabel('Y-axis', fontweight='bold')\n",
    "ax.set_zlabel('Z-axis', fontweight='bold')\n",
    "fig.colorbar(sctt, ax=ax, shrink=0.5, aspect=5)\n",
    "def update(frame):\n",
    "    ax.view_init(elev=20, azim=frame) \n",
    "    return sctt,\n",
    "animation = FuncAnimation(fig, update, frames=range(0, 360, 2), interval=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b665297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.grid(b=True, color='grey', linestyle='-.', linewidth=0.3, alpha=0.2)\n",
    "my_cmap = plt.get_cmap('hsv')\n",
    "sctt = ax.scatter3D(d_norm.dst_host_serror_rate,d_norm.dst_host_srv_count,d_norm.serror_rate, alpha=0.8,\n",
    "                    c=(d_norm.dst_host_serror_rate+d_norm.dst_host_srv_count +d_norm.serror_rate),\n",
    "                    cmap=my_cmap,\n",
    "                    marker='^')\n",
    "plt.title(\"Animated 3D Scatter Plot\")\n",
    "ax.set_xlabel('X-axis', fontweight='bold')\n",
    "ax.set_ylabel('Y-axis', fontweight='bold')\n",
    "ax.set_zlabel('Z-axis', fontweight='bold')\n",
    "fig.colorbar(sctt, ax=ax, shrink=0.5, aspect=5)\n",
    "def update(frame):\n",
    "    ax.view_init(elev=20, azim=frame)  \n",
    "    return sctt,\n",
    "animation = FuncAnimation(fig, update, frames=range(0, 360, 2), interval=50)\n",
    "html_output = animation.to_jshtml()\n",
    "HTML(html_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2709cf",
   "metadata": {},
   "source": [
    "# III. SVM :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0e21f",
   "metadata": {},
   "source": [
    "## 1) SVM for the dataset d_norm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98499813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "svm_model = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto')\n",
    "svm_model.fit(d_norm)\n",
    "predictions = svm_model.predict(d_norm)\n",
    "predictions_binary = np.where(predictions == -1, 1, 0)\n",
    "normal_count = np.sum(predictions_binary == 0)\n",
    "anomaly_count = np.sum(predictions_binary == 1)\n",
    "\n",
    "print(\"Normal instances:\", normal_count)\n",
    "print(\"Anomalous instances:\", anomaly_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eaae5a",
   "metadata": {},
   "source": [
    "## 2) SVM for the dataset d_norm_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed40882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "svm_model = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto')\n",
    "svm_model.fit(d_norm_probs)\n",
    "predictions = svm_model.predict(d_norm_probs)\n",
    "predictions_binary = np.where(predictions == -1, 1, 0)\n",
    "normal_count = np.sum(predictions_binary == 0)\n",
    "anomaly_count = np.sum(predictions_binary == 1)\n",
    "\n",
    "print(\"Normal instances:\", normal_count)\n",
    "print(\"Anomalous instances:\", anomaly_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73626a",
   "metadata": {},
   "source": [
    "## 3) SVM for the dataset d_norm_pca :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2882a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "svm_model = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto')\n",
    "svm_model.fit(d_norm_pca)\n",
    "predictions = svm_model.predict(d_norm_pca)\n",
    "predictions_binary = np.where(predictions == -1, 1, 0)\n",
    "normal_count = np.sum(predictions_binary == 0)\n",
    "anomaly_count = np.sum(predictions_binary == 1)\n",
    "\n",
    "print(\"Normal instances:\", normal_count)\n",
    "print(\"Anomalous instances:\", anomaly_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7556bb",
   "metadata": {},
   "source": [
    "## 4) SVM for the dataset d_norm_pca_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d318427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "svm_model = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto')\n",
    "svm_model.fit(d_norm_pca_probs)\n",
    "predictions = svm_model.predict(d_norm_pca_probs)\n",
    "predictions_binary = np.where(predictions == -1, 1, 0)\n",
    "normal_count = np.sum(predictions_binary == 0)\n",
    "anomaly_count = np.sum(predictions_binary == 1)\n",
    "\n",
    "print(\"Normal instances:\", normal_count)\n",
    "print(\"Anomalous instances:\", anomaly_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4aef1f",
   "metadata": {},
   "source": [
    "## 5) SVM for the dataset d_raw :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "svm_model = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto')\n",
    "svm_model.fit(d_raw)\n",
    "predictions = svm_model.predict(d_raw)\n",
    "predictions_binary = np.where(predictions == -1, 1, 0)\n",
    "normal_count = np.sum(predictions_binary == 0)\n",
    "anomaly_count = np.sum(predictions_binary == 1)\n",
    "\n",
    "print(\"Normal instances:\", normal_count)\n",
    "print(\"Anomalous instances:\", anomaly_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1672f",
   "metadata": {},
   "source": [
    "## 6) SVM for the dataset d_raw_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35394708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "svm_model = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto')\n",
    "svm_model.fit(d_raw_probs)\n",
    "predictions = svm_model.predict(d_raw_probs)\n",
    "predictions_binary = np.where(predictions == -1, 1, 0)\n",
    "normal_count = np.sum(predictions_binary == 0)\n",
    "anomaly_count = np.sum(predictions_binary == 1)\n",
    "\n",
    "print(\"Normal instances:\", normal_count)\n",
    "print(\"Anomalous instances:\", anomaly_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c2a36",
   "metadata": {},
   "source": [
    "## 7) SVM for the dataset d_raw_pca :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b68cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "svm_model = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto')\n",
    "svm_model.fit(d_raw_pca)\n",
    "predictions = svm_model.predict(d_raw_pca)\n",
    "predictions_binary = np.where(predictions == -1, 1, 0)\n",
    "normal_count = np.sum(predictions_binary == 0)\n",
    "anomaly_count = np.sum(predictions_binary == 1)\n",
    "\n",
    "print(\"Normal instances:\", normal_count)\n",
    "print(\"Anomalous instances:\", anomaly_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c702d",
   "metadata": {},
   "source": [
    "## 8) SVM for the dataset d_raw_pca_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04daa872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "svm_model = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto')\n",
    "svm_model.fit(d_raw_pca_probs)\n",
    "predictions = svm_model.predict(d_raw_pca_probs)\n",
    "predictions_binary = np.where(predictions == -1, 1, 0)\n",
    "normal_count = np.sum(predictions_binary == 0)\n",
    "anomaly_count = np.sum(predictions_binary == 1)\n",
    "\n",
    "print(\"Normal instances:\", normal_count)\n",
    "print(\"Anomalous instances:\", anomaly_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b99d16",
   "metadata": {},
   "source": [
    "## BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)\n",
    "### BIRCH est un algorithme de clustering hirarchique qui a t conu pour grer des ensembles de donnes volumineux et pour fonctionner efficacement avec des contraintes de mmoire. Voici une explication du thorme BIRCH :\n",
    "\n",
    "### Construction de la Structure CF (Clustering Feature) :\n",
    "\n",
    "BIRCH utilise une structure appele Clustering Feature (CF) pour reprsenter chaque cluster. Un CF comprend des informations agrges telles que la somme, la somme des carrs et le nombre d'lments dans le cluster. Balanced Iterative Reducing :\n",
    "\n",
    "L'algorithme suit une approche itrative pour construire et ajuster la structure CF tout en maintenant un quilibre entre la taille de la structure et la prcision du clustering. La structure est ajuste  mesure que de nouvelles donnes sont ajoutes, et les clusters peuvent fusionner si ncessaire. Utilisation de la Structure CF pour le Clustering :\n",
    "\n",
    "Une fois la structure CF construite, elle est utilise pour effectuer le clustering. Les points de donnes sont affects au cluster dont le CF est le plus proche."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572b18f",
   "metadata": {},
   "source": [
    "## d_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "d_raw_birch = d_raw.copy()\n",
    "birch = Birch(branching_factor=50, threshold=0.5, n_clusters=None)\n",
    "birch.fit(d_raw_birch)\n",
    "y_birch = birch.predict(d_raw_birch)\n",
    "d_raw_np = d_raw.to_numpy()\n",
    "plt.scatter(d_raw_np[:, 0], d_raw_np[:, 1], c=y_birch, s=40, cmap='viridis')\n",
    "plt.title('Clustering avec BIRCH')\n",
    "plt.show()\n",
    "train_f_birch = d_raw[y_birch > 0]\n",
    "train_c_birch = d_raw[y_birch <= 0]\n",
    "unique_clusters = np.unique(y_birch)\n",
    "num_clusters = len(unique_clusters)\n",
    "print(\"Nombre de clusters :\", num_clusters)\n",
    "print(\"Clusters uniques :\", unique_clusters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d04dc4",
   "metadata": {},
   "source": [
    "## d_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "d_raw_birch1 = d_norm.copy()\n",
    "birch = Birch(branching_factor=50, threshold=0.5, n_clusters=None)\n",
    "birch.fit(d_raw_birch1)\n",
    "y_birch = birch.predict(d_raw_birch1)\n",
    "d_norm_np = d_norm.to_numpy()\n",
    "plt.scatter(d_norm_np[:, 0], d_norm_np[:, 1], c=y_birch, s=40, cmap='viridis')\n",
    "plt.title('Clustering avec BIRCH')\n",
    "plt.show()\n",
    "train_f_birch = d_norm[y_birch > 0]\n",
    "train_c_birch = d_norm[y_birch <= 0]\n",
    "unique_clusters = np.unique(y_birch)\n",
    "num_clusters = len(unique_clusters)\n",
    "print(\"Nombre de clusters aprs normalisation:\", num_clusters)\n",
    "print(\"Clusters uniques aprs normalisation :\", unique_clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38e913",
   "metadata": {},
   "source": [
    "### d_norm_pca : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443fd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "d_norm_birch2 = d_norm_pca.copy()\n",
    "birch = Birch(branching_factor=50, threshold=0.5, n_clusters=None)\n",
    "birch.fit(d_norm_birch2)\n",
    "y_birch = birch.predict(d_norm_birch2)\n",
    "d_norm_pca_np = d_norm_pca.to_numpy()\n",
    "plt.scatter(d_norm_pca_np[:, 0], d_norm_pca_np[:, 1], c=y_birch, s=40, cmap='viridis')\n",
    "plt.title('Clustering avec BIRCH')\n",
    "plt.show()\n",
    "train_f_birch = d_norm_pca[y_birch > 0]\n",
    "train_c_birch = d_norm_pca[y_birch <= 0]\n",
    "unique_clusters = np.unique(y_birch)\n",
    "num_clusters = len(unique_clusters)\n",
    "print(\"Nombre de clusters aprs pca :\", num_clusters)\n",
    "print(\"Clusters uniques aprs pca:\", unique_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559784f9",
   "metadata": {},
   "source": [
    "## d_raw_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.pyplot as plt\n",
    "d_raw_probs_birch2 = d_raw_probs.copy()\n",
    "birch = Birch(branching_factor=50, threshold=0.5, n_clusters=None)\n",
    "birch.fit(d_raw_probs_birch2)\n",
    "y_birch = birch.predict(d_raw_probs_birch2)\n",
    "d_raw_probs_np =d_raw_probs.to_numpy()\n",
    "plt.scatter(d_raw_probs_np[:, 0],d_raw_probs_np[:, 1], c=y_birch, s=40, cmap='viridis')\n",
    "plt.title('Clustering avec BIRCH')\n",
    "plt.show()\n",
    "train_f_birch =d_raw_probs[y_birch > 0]\n",
    "train_c_birch = d_raw_probs[y_birch <= 0]\n",
    "unique_clusters = np.unique(y_birch)\n",
    "num_clusters = len(unique_clusters)\n",
    "print(\"Nombre de clusters :\", num_clusters)\n",
    "print(\"Clusters uniques:\", unique_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d42a5",
   "metadata": {},
   "source": [
    "## d_raw_pca_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.pyplot as plt\n",
    "d_raw_pca_birch2 = d_raw_pca_probs.copy()\n",
    "birch = Birch(branching_factor=50, threshold=0.5, n_clusters=None)\n",
    "birch.fit(d_raw_pca_birch2)\n",
    "y_birch = birch.predict(d_raw_pca_birch2)\n",
    "plt.scatter(d_raw_pca_probs[:, 0],d_raw_pca_probs[:, 1], c=y_birch, s=40, cmap='viridis')\n",
    "plt.title('Clustering avec BIRCH')\n",
    "plt.show()\n",
    "train_f_birch =d_raw_pca_probs[y_birch > 0]\n",
    "train_c_birch = d_raw_pca_probs[y_birch <= 0]\n",
    "unique_clusters = np.unique(y_birch)\n",
    "num_clusters = len(unique_clusters)\n",
    "print(\"Nombre de clusters :\", num_clusters)\n",
    "print(\"Clusters uniques:\", unique_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744e6be",
   "metadata": {},
   "source": [
    "## d_norm_probs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5201464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.pyplot as plt\n",
    "d_norm_probs_birch2 = d_norm_probs.copy()\n",
    "birch = Birch(branching_factor=50, threshold=0.5, n_clusters=None)\n",
    "birch.fit(d_norm_probs_birch2)\n",
    "y_birch = birch.predict(d_norm_probs_birch2)\n",
    "plt.scatter(d_norm_probs[:, 0],d_norm_probs[:, 1], c=y_birch, s=40, cmap='viridis')\n",
    "plt.title('Clustering avec BIRCH')\n",
    "plt.show()\n",
    "train_f_birch =d_norm_probs[y_birch > 0]\n",
    "train_c_birch = d_norm_probs[y_birch <= 0]\n",
    "unique_clusters = np.unique(y_birch)\n",
    "num_clusters = len(unique_clusters)\n",
    "print(\"Nombre de clusters  :\", num_clusters)\n",
    "print(\"Clusters uniques:\", unique_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe7868",
   "metadata": {},
   "source": [
    "## d_norm_pca_probs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26546cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import Birch\n",
    "import matplotlib.pyplot as plt\n",
    "d_norm_pca_birch2 = d_norm_pca_probs.copy()\n",
    "birch = Birch(branching_factor=50, threshold=0.5, n_clusters=None)\n",
    "birch.fit(d_norm_pca_birch2)\n",
    "y_birch = birch.predict(d_norm_pca_birch2)\n",
    "plt.scatter(d_norm_pca_probs[:, 0],d_norm_pca_probs[:, 1], c=y_birch, s=40, cmap='viridis')\n",
    "plt.title('Clustering avec BIRCH')\n",
    "plt.show()\n",
    "train_f_birch =d_norm_pca_probs[y_birch > 0]\n",
    "train_c_birch = d_norm_pca_probs[y_birch <= 0]\n",
    "unique_clusters = np.unique(y_birch)\n",
    "num_clusters = len(unique_clusters)\n",
    "print(\"Nombre de clusters aprs pca :\", num_clusters)\n",
    "print(\"Clusters uniques aprs pca:\", unique_clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315952f7",
   "metadata": {},
   "source": [
    "## V. MLP : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c521c61",
   "metadata": {},
   "source": [
    "## 1) MLP on d_raw :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39adc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "X, _ = make_classification(n_samples=10000, n_features=24, n_classes=2, random_state=42)\n",
    "autoencoder = MLPRegressor(hidden_layer_sizes=(24,), max_iter=10000, random_state=42)\n",
    "autoencoder.fit(d_raw, d_raw)\n",
    "encoded_data = autoencoder.predict(d_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)\n",
    "mlp_regressor.fit(d_raw, d_raw) \n",
    "plt.scatter(encoded_data[:, 0], encoded_data[:, 1], label='Encoded Data')\n",
    "plt.title('2D Representation of Encoded Data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0f420",
   "metadata": {},
   "source": [
    "## 2) MLP on d_norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "X, _ = make_classification(n_samples=10000, n_features=24, n_classes=2, random_state=42)\n",
    "autoencoder = MLPRegressor(hidden_layer_sizes=(24,), max_iter=10000, random_state=42)\n",
    "autoencoder.fit(d_norm, d_norm)\n",
    "encoded_data1= autoencoder.predict(d_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)\n",
    "mlp_regressor.fit(d_norm, d_norm) \n",
    "plt.scatter(encoded_data1[:, 0], encoded_data1[:, 1], label='Encoded Data')\n",
    "plt.title('2D Representation of Encoded Data')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bc23b2",
   "metadata": {},
   "source": [
    "## 3) MLP on d_norm_pca :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "X, _ = make_classification(n_samples=10000, n_features=24, n_classes=2, random_state=42)\n",
    "autoencoder = MLPRegressor(hidden_layer_sizes=(24,), max_iter=10000, random_state=42)\n",
    "autoencoder.fit(d_norm_pca, d_norm_pca)\n",
    "encoded_data2= autoencoder.predict(d_norm_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)\n",
    "mlp_regressor.fit(d_norm_pca,d_norm_pca) \n",
    "plt.scatter(encoded_data2[:, 0], encoded_data2[:, 1], label='Encoded Data')\n",
    "plt.title('2D Representation of Encoded Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8f0bd",
   "metadata": {},
   "source": [
    "## 4) MLP on  d_raw_probs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a366f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "X, _ = make_classification(n_samples=10000, n_features=24, n_classes=2, random_state=42)\n",
    "autoencoder = MLPRegressor(hidden_layer_sizes=(24,), max_iter=10000, random_state=42)\n",
    "autoencoder.fit(d_raw_probs,d_raw_probs)\n",
    "encoded_data3= autoencoder.predict(d_raw_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56eeba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42)\n",
    "mlp_regressor.fit(d_raw_probs,d_raw_probs) \n",
    "plt.scatter(encoded_data3[:, 0], encoded_data3[:, 1], label='Encoded Data')\n",
    "plt.title('2D Representation of Encoded Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f706e65",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.75em;color:red; font-style:bold\"><br>\n",
    "IV. Evaluation:</p><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7530197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data_sets = {\n",
    "    'd_raw': d_raw,\n",
    "    'd_raw_probs': d_raw_probs,\n",
    "    'd_raw_pca': d_raw_PCA,\n",
    "    'd_raw_pca_probs': d_raw_pca_probs,\n",
    "    'd_norm': d_norm,\n",
    "    'd_norm_probs': d_norm_probs,\n",
    "    'd_norm_pca': d_norm_pca,\n",
    "    'd_norm_pca_probs': d_norm_pca_probs,\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "fig.suptitle('Clustering Results and Silhouette Scores')\n",
    "\n",
    "silhouette_scores = {}\n",
    "\n",
    "for i, (data_name, data) in enumerate(data_sets.items()):\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    labels = kmeans.predict(data)\n",
    "    silhouette_avg = silhouette_score(data, labels)\n",
    "    silhouette_scores[data_name] = silhouette_avg\n",
    "    reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "    axes[i // 4, i % 4].scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='viridis')\n",
    "    axes[i // 4, i % 4].set_title(f'{data_name}\\nSilhouette Score: {silhouette_avg:.2f}')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
